{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import matplotlib_venn as venn\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "\n",
    "\n",
    "import warnings\n",
    "color = sns.color_palette()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "tokenizer = TweetTokenizer()\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'train.csv')\n",
    "test = pd.read_csv(r'test.csv')\n",
    "# sub = pd.read_csv(r'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these comments that do not have any flags are actually clean. Let's determine how many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowsums = train.iloc[:, 2:].sum(axis=1)\n",
    "train['clean'] = (rowsums == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of clean comments: {:,} or {:.2%}'.format(train['clean'].sum(), train['clean'].sum() / train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['comment_text'].loc[train['toxic'] == 1][:1].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peak at some of these comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display one comment from each label\n",
    "for i in train.columns[2:]:\n",
    "    print('[{}] \\t {}'.format(i, train['comment_text'].loc[train[i] == 1][:1].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a bunch of meanies.\n",
    "\n",
    "It looks like a comment can have multiple classifications (ie being toxic as well as obscene and an insult)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].iloc[:].sum(axis=0)\n",
    "x = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'clean']].iloc[:].sum(axis=0)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,6))\n",
    "a = sns.barplot(w.index, w.values, ax=ax[0])\n",
    "b = sns.barplot(x.index, x.values, ax=ax[1])\n",
    "\n",
    "rects = b.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    b.text(rect.get_x() + rect.get_width() / 2, height + 5, label, ha='center', va='bottom')\n",
    "    \n",
    "rects = a.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    a.text(rect.get_x() + rect.get_width() / 2, height + 5, label, ha='center', va='bottom')\n",
    "    \n",
    "a.set_ylabel('Count', fontsize=14)\n",
    "a.set_xlabel('Label ', fontsize=14)\n",
    "b.set_xlabel('Label ', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the label counts are not evenly distributed. Toxic comments heavily outweight any other label, while threats are quite rare.\n",
    "\n",
    "However, toxic comments may have multiple labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rowsums.value_counts()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(16,6))\n",
    "a = sns.barplot(x.index, x.values)\n",
    "\n",
    "\n",
    "rects = a.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    a.text(rect.get_x() + rect.get_width() / 2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "a.set_title('Multiple Label Counts')\n",
    "a.set_ylabel('Count', fontsize=14)\n",
    "a.set_xlabel('Number of multi-label counts ', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_col = \"toxic\"\n",
    "corr_mats = []\n",
    "temp_df = train.iloc[:, 2:-1]\n",
    "\n",
    "for other_col in temp_df.columns[1:]:\n",
    "    confusion_matrix = pd.crosstab(temp_df[main_col], temp_df[other_col])\n",
    "    corr_mats.append(confusion_matrix)\n",
    "out = pd.concat(corr_mats,axis=1,keys=temp_df.columns[1:])\n",
    "\n",
    "# cell highlighting\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the minimum in a Series yellow.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_min]\n",
    "\n",
    "\n",
    "\n",
    "out = out.style.apply(highlight_min, axis=0)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above chart which shows the counts of labels, we see that:\n",
    "    - toxic = 15294\n",
    "    - severe_toxic = 1595\n",
    "    - obscene = 8449\n",
    "    - threat = 478\n",
    "    - insult = 7877\n",
    "    - identity_hate = 1405\n",
    "    \n",
    "So now from the confusion matrix above, we can see that:\n",
    "1. A severe_toxic comment is **always** toxic\n",
    "2. Almost all obscene comments are toxic\n",
    "3. Almost all threats, insults, and identity are toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud - Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# Initalize stopwords\n",
    "stopword = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfclean = train[train['clean'] == True]\n",
    "cleanComments = dfclean['comment_text'].values\n",
    "\n",
    "cloud = WordCloud(background_color='black', max_words=2000, stopwords=stopword)\n",
    "cloud.generate(\" \".join(cleanComments))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Frequent Words - Clean Comments\", fontsize=20)\n",
    "plt.imshow(cloud.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftoxic = train.loc[train['toxic'] == 1]\n",
    "toxicComments = dftoxic['comment_text'].values\n",
    "\n",
    "cloud = WordCloud(background_color='black', max_words=2000, stopwords=stopword)\n",
    "cloud.generate(\" \".join(toxicComments))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Frequent Words - Toxic Comments\", fontsize=20)\n",
    "plt.imshow(cloud.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSeveretoxic = train.loc[train['severe_toxic'] == 1]\n",
    "SeveretoxicComments = dfSeveretoxic['comment_text'].values\n",
    "\n",
    "cloud = WordCloud(background_color='black', max_words=2000, stopwords=stopword)\n",
    "cloud.generate(\" \".join(SeveretoxicComments))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Frequent Words - Severe Toxic Comments\", fontsize=20)\n",
    "plt.imshow(cloud.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfobscene = train.loc[train['obscene'] == 1]\n",
    "obsceneComments = dfobscene['comment_text'].values\n",
    "\n",
    "cloud = WordCloud(background_color='black', max_words=2000, stopwords=stopword)\n",
    "cloud.generate(\" \".join(obsceneComments))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Frequent Words - Obscene Comments\", fontsize=20)\n",
    "plt.imshow(cloud.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfthreat = train.loc[train['threat'] == 1]\n",
    "threatComments = dfobscene['comment_text'].values\n",
    "\n",
    "cloud = WordCloud(background_color='black', max_words=2000, stopwords=stopword)\n",
    "cloud.generate(\" \".join(threatComments))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Frequent Words - Threat Comments\", fontsize=20)\n",
    "plt.imshow(cloud.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinsult = train.loc[train['insult'] == 1]\n",
    "insultComments = dfinsult['comment_text'].values\n",
    "\n",
    "cloud = WordCloud(background_color='black', max_words=2000, stopwords=stopword)\n",
    "cloud.generate(\" \".join(insultComments))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Frequent Words - Insult Comments\", fontsize=20)\n",
    "plt.imshow(cloud.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfhate = train.loc[train['identity_hate'] == 1]\n",
    "hateComments = dfhate['comment_text'].values\n",
    "\n",
    "cloud = WordCloud(background_color='black', max_words=2000, stopwords=stopword)\n",
    "cloud.generate(\" \".join(hateComments))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Frequent Words - Hate Comments\", fontsize=20)\n",
    "plt.imshow(cloud.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train & test dataset and reset index\n",
    "\n",
    "merged = pd.concat([train.iloc[:, 0:2], test.iloc[:, 0:2]])\n",
    "df = merged.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we remove punctuation, we will end up removing complete sentences. Instead we will count various metrics in order to further understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of new lines '\\n'\n",
    "df['sentenceCount'] = df['comment_text'].apply(lambda x: len(re.findall(\"\\n\", str(x))) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words \n",
    "df['wordCount'] = df['comment_text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Unique number of words\n",
    "df['uniqueWordCount'] = df['comment_text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# Number of letters\n",
    "df['letterCount'] = df['comment_text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Punctuation count\n",
    "df['puncCount'] = df['comment_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# Number of uppercase words\n",
    "df[\"uppercaseCount\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "# Number of titled words (words starting with capital letter)\n",
    "df[\"titleWordCount\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "# Number of stopwords\n",
    "df[\"stopwordCount\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopword]))\n",
    "\n",
    "# Average word length\n",
    "df[\"meanWordLength\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# Unique word count percentage\n",
    "df['wordCountPercent'] = np.round(df['uniqueWordCount'] / df['wordCount'] * 100, 2)\n",
    "\n",
    "# Punctuation percentage per comment\n",
    "df['puncPercent'] = np.round(df['puncCount'] / df['wordCount'] * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Train/Test data\n",
    "trainFeatures = df.iloc[0:len(train)]\n",
    "testFeatures = df.iloc[len(train) :,]\n",
    "\n",
    "# Join tags\n",
    "trainTags = train.iloc[:, 2:]\n",
    "trainFeatures = pd.concat([trainFeatures, trainTags], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are comments with more punctuation more toxic?\n",
    "- Are longer comments more toxic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures.loc[df['wordCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures[['wordCount', 'toxic']].sort_values(by='wordCount', ascending=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures['wordCount'].loc[trainFeatures['toxic'] == 0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(16,10))\n",
    "a = sns.violinplot(x='toxic', y='wordCount', data=trainFeatures, ax=ax[0])\n",
    "b = sns.violinplot(x='toxic', y='puncCount', data=trainFeatures, ax=ax[1])\n",
    "\n",
    "a.set_title('Word Count vs Toxicity', fontsize=14)\n",
    "b.set_title('Punc Count vs Toxicity', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per violin plots, neither word count nor punctuation count seems to affect toxicity. There are huge outliers which may indicate toxic comments are more 'spammy'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine 'spam' by looking at word count versus unique word count. When the ratio of unique word count to total word count is low, it would indicate spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures['spamRatio'] = 1 - np.round(trainFeatures['uniqueWordCount'] / trainFeatures['wordCount'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures.loc[trainFeatures['spamRatio'] == 1][:5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what this comment looks like (first 250 characters)\n",
    "trainFeatures.iloc[2420][1][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures.iloc[8705][1][:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure does look like spam - and toxic as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(16,10))\n",
    "c = sns.violinplot(x='toxic', y='spamRatio', data=trainFeatures.loc[trainFeatures['spamRatio'] > 0.75], ax=ax[0])\n",
    "d = sns.violinplot(x='toxic', y='wordCount', data=trainFeatures.loc[trainFeatures['spamRatio'] > 0.75], ax=ax[1])\n",
    "\n",
    "c.set_title('Spam Ratio vs Toxicity', fontsize=14)\n",
    "d.set_title('Word Count vs Toxicity', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, spam coincides with toxicity. Spammers are more toxic!\n",
    "\n",
    "However, it's important to note that it is possible to spam and not be toxic. Let's see what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures.loc[(trainFeatures['spamRatio'] > 0.75) & (trainFeatures['toxic'] != 1)][:5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures.iloc[2567][1][:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, spam becomes toxic to our model too.\n",
    "\n",
    "Let's create our own Count Vectorizer to pick up specific items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky features\n",
    "df['IP'] = df[\"comment_text\"].apply(lambda x: re.findall(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", str(x)))\n",
    "\n",
    "# Count of IP addresses\n",
    "df['countIP'] = df[\"IP\"].apply(lambda x: len(x))\n",
    "\n",
    "# URLs\n",
    "df['URL'] = df[\"comment_text\"].apply(lambda x: re.findall(\"http://.*com\",str(x)))\n",
    "\n",
    "# Count of URLs\n",
    "df['countURL'] = df[\"URL\"].apply(lambda x: len(x))\n",
    "\n",
    "# Article IDs\n",
    "df['articleID'] = df[\"comment_text\"].apply(lambda x: re.findall(\"\\d:\\d\\d\\s{0,5}$\",str(x)))\n",
    "df['articleIDFlag'] = df['articleID'].apply(lambda x: len(x))\n",
    "\n",
    "# Username\n",
    "df['username'] = df[\"comment_text\"].apply(lambda x: re.findall(\"\\[\\[User(.*)\\|\",str(x)))\n",
    "\n",
    "#count of username mentions\n",
    "df['countUsernames'] = df[\"username\"].apply(lambda x: len(x))\n",
    "\n",
    "# check if features are created\n",
    "# df.username[df.count_usernames>0]\n",
    "\n",
    "# Leaky IP\n",
    "cv = CountVectorizer()\n",
    "count_feats_ip = cv.fit_transform(df[\"IP\"].apply(lambda x : str(x)))\n",
    "\n",
    "\n",
    "# Leaky usernames\n",
    "\n",
    "cv = CountVectorizer()\n",
    "count_feats_user = cv.fit_transform(df[\"username\"].apply(lambda x : str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check some feature names\n",
    "\n",
    "cv.get_feature_names()[100:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be useful to determine if we have a lot of reocurring features - such as IPs and URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_feats = df[[\"IP\", \"URL\", \"articleID\", \"username\", \"countIP\",\"countURL\",\"countUsernames\",\"articleIDFlag\"]]\n",
    "\n",
    "leaky_feats_train = leaky_feats.iloc[:train.shape[0]]\n",
    "leaky_feats_test = leaky_feats.iloc[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib_venn as venn\n",
    "\n",
    "# Filter out items that do not contain IPs\n",
    "train_IPs = leaky_feats_train.loc[(leaky_feats_train['IP'].str.len() != 0)]\n",
    "test_IPs = leaky_feats_test.loc[(leaky_feats_test['IP'].str.len() != 0)]\n",
    "\n",
    "# Obtain list of unique IPs\n",
    "train_IP_list = list(set([a for b in train_IPs['IP'].tolist() for a in b]))\n",
    "test_IP_list = list(set([a for b in test_IPs['IP'].tolist() for a in b]))\n",
    "\n",
    "# Obtain common elements\n",
    "common_IP_list = list(set(train_IP_list).intersection(test_IP_list))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Common IP addresses\")\n",
    "venn.venn2(subsets=(len(train_IP_list), len(test_IP_list), len(common_IP_list)),\n",
    "           set_labels=(\"# of unique IPs (Train)\", \"# of unique IPs (Test)\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out items without URLs\n",
    "train_URLs = leaky_feats_train.loc[(leaky_feats_train['URL'].str.len() != 0)]\n",
    "test_URLs = leaky_feats_test.loc[(leaky_feats_test['URL'].str.len() != 0)]\n",
    "\n",
    "# Obtain list of unique URLs\n",
    "train_URLs_list = list(set([a for b in train_IPs['URL'].tolist() for a in b]))\n",
    "test_URLs_list = list(set([a for b in test_IPs['URL'].tolist() for a in b]))\n",
    "\n",
    "# Obtain common elements\n",
    "common_URLs_list=list(set(train_URLs_list).intersection(test_URLs_list))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Common URLs\")\n",
    "venn.venn2(subsets=(len(train_URLs_list),len(test_URLs_list),len(common_URLs_list)),\n",
    "           set_labels=(\"# of unique URLs (Train)\",\"# of unique URLs (Test)\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out items without usernames\n",
    "train_users = leaky_feats_train['username'].loc[(leaky_feats_train['countUsernames'] != 0)]\n",
    "test_users = leaky_feats_test['username'].loc[(leaky_feats_test['countUsernames'] != 0)]\n",
    "\n",
    "# Obtain list of unique usernames\n",
    "train_users_list = list(set([a for b in train_users.tolist() for a in b]))\n",
    "test_users_list = list(set([a for b in test_users.tolist() for a in b]))\n",
    "                        \n",
    "# Obtain common elements\n",
    "common_users_list = list(set(train_users_list).intersection(test_users_list))\n",
    "                       \n",
    "plt.title(\"Common usernames\")\n",
    "venn.venn2(subsets=(len(train_users_list),len(test_users_list),len(common_users_list)),\n",
    "           set_labels=(\"# of unique usernames (Train)\",\"# of unique usernames (Test)\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature stability (or reoccurance) of train dataset usernames in the test dataset seems to be minimal. \n",
    "\n",
    "Therefore can just use the IPs/URLs in common (intersection) for test and train in our feature engineering.\n",
    "\n",
    "Note that it may be useful to look more into these IPs - for example, there may be invalid or blocked IPs present that we would not want to interfere with our model (https://en.wikipedia.org/wiki/Wikipedia:Database_reports/Indefinitely_blocked_IPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aphost lookup dict\n",
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = merged['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanComment(comment):\n",
    "    \"\"\"Takes a comment and returns cleaned copy.\"\"\"\n",
    "    \n",
    "    # Lower case\n",
    "    comment = comment.lower()\n",
    "    \n",
    "    # Remove '\\n'\n",
    "    comment = re.sub('\\\\n', '', comment)\n",
    "    \n",
    "    # Remove IPs\n",
    "    comment = re.sub('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', comment)\n",
    "    \n",
    "    # Remove username\n",
    "    comment = re.sub('\\[\\[.*\\]', '', comment)\n",
    "    \n",
    "    # Split comment (sentences) into words (tokens)\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    \n",
    "    # Replace apostrophes; you're --> you are  \n",
    "    # using basic dictionary lookup \n",
    "    words = [APPO[word] if word in APPO else word for word in words]\n",
    "    words = [lem.lemmatize(word, 'v') for word in words]\n",
    "    words = [w for w in words if not w in eng_stopwords]\n",
    "    \n",
    "    cleaned = ' '.join(words)\n",
    "\n",
    "    return(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.iloc[12235]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanComment(corpus.iloc[12235])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean entire corpus\n",
    "\n",
    "%time clean_corpus = corpus.apply(lambda x: cleanComment(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Features\n",
    "\n",
    "- Count Based Features (unigrams)\n",
    "\n",
    "Let's create some features based on the frequency distribution of the words. We can start by taking words one at a time (unigrams).\n",
    "\n",
    "- CountVectorizer \n",
    "    - Creates a matrix with frequency counts of each word in the text corpus\n",
    "- TF IDF Vectorizer\n",
    "    - Term Frequency: Count of the words (terms) in the corpus (same as CountVectorizer)\n",
    "    - Inverse Document Frequency: Penalizes words that are too frequent (can be thought of as regularization)\n",
    "- HashingVectorizer\n",
    "    - Creates a hashmap (word to number mapping based on hashing technique) instead of a dictionary for words\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://buhrmann.github.io/tfidf-analysis.html\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    \"\"\"Get top n tfidf values in row and return them with their corresponding feature names.\"\"\"\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    \"\"\"Top tfidf features in specific document (matrix row)\"\"\"\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids, min_tfidf=0.1, top_n=25):\n",
    "    \"\"\"Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids.\"\"\"\n",
    "    \n",
    "    D = Xtr[grp_ids].toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "# modified for multilabel milticlass\n",
    "def top_feats_by_class(Xtr, features, min_tfidf=0.1, top_n=20):\n",
    "    \"\"\"Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "       calculated across documents with the same class label.\"\"\"\n",
    "    dfs = []\n",
    "    cols=train_tags.columns\n",
    "    for col in cols:\n",
    "        ids = train_tags.index[train_tags[col]==1]\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Descriptions\n",
    "\n",
    "# min_df=10; ignores terms that appear less than 10 times\n",
    "# max_features=None; creates as many words as present in the corpus, restricting to 10k for memory capacity\n",
    "# analyzer='word'; creates features from words \n",
    "# ngram_range=(1,1); use only one word at a time (ie unigram)\n",
    "# strip_accents='unicode'; removes accents\n",
    "# use_idf=1, smooth_idf=1; enables IDF\n",
    "# sublinear_tf=1; apply sublinear scaling - ie replace tf with 1 + log(tf)\n",
    "\n",
    "\n",
    "startUnigrams = time.time()\n",
    "\n",
    "tfv = TfidVectorizer(min_df=10, max_features=10000, strip_accents='unicode', analyzer='word', ngram_range=(1,1),\n",
    "                     use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "tfv.fit(clean_corpus)\n",
    "features = np.array(tfv.get_feature_names())\n",
    "\n",
    "train_unigrams = tfv.transform(clean_corpus.iloc[:train.shape[0]])\n",
    "test_unigrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch top 'n' for Unigrams\n",
    "\n",
    "tfidf_top_n_per_class = top_feats_by_class(train_unigrams, features)\n",
    "\n",
    "endUnigrams = time.time()\n",
    "\n",
    "print(\"Time to compute unigrams: {:.2f}\".format(endUnigrams - startUnigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 22))\n",
    "plt.suptitle(\"TF-IDF Top Words Per Class (Unigrams)\", fontsize=20)\n",
    "gridspec.GridSpec(4,2)\n",
    "plt.subplot2grid((4,2), (0,0))\n",
    "sns.barplot(tfidf_top_n_per_class[0].feature.iloc[0:9], tfidf_top_n_per_class[0].tfidf.iloc[0:9], color=color[0])\n",
    "plt.title(\"Class: Toxic\", fontsize=15)\n",
    "plt.xlabel(\"Word\", fontsize=12)\n",
    "plt.ylabel(\"TF-IDF Score\", fontsize=12)\n",
    "\n",
    "plt.subplot2grid((4,2), (0,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:9],tfidf_top_n_per_lass[1].tfidf.iloc[0:9],color=color[1])\n",
    "plt.title(\"Class: Severe Toxic\", fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2), (1,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[2].feature.iloc[0:9],tfidf_top_n_per_lass[2].tfidf.iloc[0:9],color=color[2])\n",
    "plt.title(\"Class: Obscene\", fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2), (1,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[3].feature.iloc[0:9],tfidf_top_n_per_lass[3].tfidf.iloc[0:9],color=color[3])\n",
    "plt.title(\"Class: Threat\", fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2), (2,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[4].feature.iloc[0:9],tfidf_top_n_per_lass[4].tfidf.iloc[0:9],color=color[4])\n",
    "plt.title(\"Class: Insult\", fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(2,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[5].feature.iloc[0:9],tfidf_top_n_per_lass[5].tfidf.iloc[0:9],color=color[5])\n",
    "plt.title(\"Class: Identity Hate\", fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(3,0),colspan=2)\n",
    "sns.barplot(tfidf_top_n_per_lass[6].feature.iloc[0:19],tfidf_top_n_per_lass[6].tfidf.iloc[0:19])\n",
    "plt.title(\"Class: Clean\", fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same analysis - except for Bigrams this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startBigrams = time.time()\n",
    "\n",
    "# Set min to 150 to quickly get top features; change back to 10 for better results\n",
    "tfv = TfidVectorizer(min_df=10, max_features=30000, strip_accents='unicode', analyzer='word', ngram_range=(2,2),\n",
    "                     use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "\n",
    "tfv.fit(clean_corpus)\n",
    "features = np.array(tfv.get_feature_names())\n",
    "\n",
    "train_bigrams = tfv.transform(clean_corpus.iloc[:train.shape[0]])\n",
    "test_bigrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])\n",
    "\n",
    "\n",
    "\n",
    "# Fetch top 'n' Bigrams\n",
    "tfidf_top_n_per_class = top_feats_by_class(train_unigrams, features)\n",
    "\n",
    "endBigrams = time.time()\n",
    "\n",
    "print(\"Time to compute bigrams: {:.2f}\".format(endBigrams - startBigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 22))\n",
    "plt.suptitle(\"TF-IDF Top Words Per Class (Bigrams)\", fontsize=20)\n",
    "gridspec.GridSpec(4,2)\n",
    "plt.subplot2grid((4,2), (0,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:5],tfidf_top_n_per_lass[0].tfidf.iloc[0:5],color=color[0])\n",
    "plt.title(\"Class: Toxic\", fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "plt.subplot2grid((4,2), (0,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:5],tfidf_top_n_per_lass[1].tfidf.iloc[0:5],color=color[1])\n",
    "plt.title(\"Class: Severe Toxic\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2), (1,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[2].feature.iloc[0:5],tfidf_top_n_per_lass[2].tfidf.iloc[0:5],color=color[2])\n",
    "plt.title(\"Class: Obscene\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2), (1,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[3].feature.iloc[0:5],tfidf_top_n_per_lass[3].tfidf.iloc[0:5],color=color[3])\n",
    "plt.title(\"Class: Threat\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2), (2,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[4].feature.iloc[0:5],tfidf_top_n_per_lass[4].tfidf.iloc[0:5],color=color[4])\n",
    "plt.title(\"Class: Insult\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2), (2,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[5].feature.iloc[0:5],tfidf_top_n_per_lass[5].tfidf.iloc[0:5],color=color[5])\n",
    "plt.title(\"Class: Identity Hate\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2), (3,0), colspan=2)\n",
    "sns.barplot(tfidf_top_n_per_lass[6].feature.iloc[0:9],tfidf_top_n_per_lass[6].tfidf.iloc[0:9])\n",
    "plt.title(\"Class: Clean\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF Score', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do it again for character-ngrams (specifically 1 x 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startChargrams = time.time()\n",
    "\n",
    "tfv = TfidVectorizer(min_df=10, max_features=30000, strip_accents='unicode', analyzer='char', ngram_range=(1,4),\n",
    "                     use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "\n",
    "tfv.fit(clean_corpus)\n",
    "features = np.array(tfv.get_feature_names())\n",
    "\n",
    "train_chargrams = tfv.transform(clean_corpus.iloc[:train.shape[0]])\n",
    "test_chargrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])\n",
    "\n",
    "\n",
    "\n",
    "# Fetch top 'n' Bigrams\n",
    "tfidf_top_n_per_class = top_feats_by_class(train_unigrams, features)\n",
    "\n",
    "endChargrams = time.time()\n",
    "\n",
    "print(\"Time to compute bigrams: {:.2f}\".format(endChargrams - startChargrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_COLS = ['sentenceCount', 'wordCount', 'uniqueWordCount', 'letterCount', \n",
    "                 'puncCount', 'uppercaseCount', 'titleWordCount','stopwordCount',\n",
    "                 'meanWordLength', 'wordCountPercent', 'puncPercent']\n",
    "\n",
    "target_x = trainFeatures[SELECTED_COLS]\n",
    "\n",
    "TARGET_COLS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "target_y = trainTags[TARGET_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
