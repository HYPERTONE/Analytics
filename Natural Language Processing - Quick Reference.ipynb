{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization - processing of converting text into 'tokens'\n",
    "2. Tokens - words or entities present in text\n",
    "3. Text Object - a sentence, phrase, or word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing\n",
    "\n",
    "Preprocessing involes 'cleaning' the text and (hopefully) making it as noise free as possible.\n",
    "\n",
    "1. Noise removal\n",
    "2. Lexicon normalization\n",
    "3. Object standardization\n",
    "4. Grammar check\n",
    "5. Spellcheck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Removal\n",
    "\n",
    "Stopwords are generally the most common words in a language - things like 'the', 'is', 'at', 'a', 'which'. They can also be hashtags, tinyurls, or unwanted ascii characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Steve bar.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def removeNoise(s):\n",
    "    \"\"\"Takes a string and removes the noise.\"\"\"\n",
    "    \n",
    "    noise = ['the', 'is', 'at', 'a', 'which']\n",
    "\n",
    "    a = []\n",
    "    \n",
    "    for index, value in enumerate(s.split()):\n",
    "        if value not in noise:\n",
    "            a.append(value)\n",
    "            \n",
    "    return \" \".join(a)\n",
    "\n",
    "removeNoise('Steve is at the bar.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like to remove punctuation. We can do that using regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Steve', 'bar']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def removeNoise(s):\n",
    "    \"\"\"Takes a string and removes the noise.\"\"\"\n",
    "    \n",
    "    noise = ['the', 'is', 'at', 'a', 'which']\n",
    "\n",
    "    # Substitute all non alphanumeric characters and spaces with an empty string\n",
    "    s = re.sub(r'[^\\w\\s]','',s) \n",
    "    \n",
    "    a = []\n",
    "    \n",
    "    for index, value in enumerate(s.split()):\n",
    "        if value not in noise:\n",
    "            a.append(value)\n",
    "    \n",
    "    \n",
    "    return a\n",
    "\n",
    "removeNoise('Steve is at the bar.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a suffix to a word does not change the root context of the word. For example 'running' and 'runs' all have the same root of 'run'. Likewise 'play', 'player', 'played', 'playing', and 'plays' have the same root of 'play'.\n",
    "\n",
    "- Stemming is the process of stripping suffixes ('ing', 's', 'es', 'ly, 'y', etc.)\n",
    "- Lemmatization is the process of obtaining the root of the word\n",
    "\n",
    "We can use the Natural Language Toolkit to carry out these tasks (https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "print(stemmer.stem('programming'))\n",
    "print(lemmatizer.lemmatize('running', 'v')) # where a=adjective and v=verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we abbreviate or truncate words and use acronyms. These are not necessarily noise and can be valuable pieces of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what are you doing Charlie, direct message me'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = {'wyd':'what are you doing', 'dm':'direct message', 'rt' : 'retweet'}\n",
    "# We can continue to append words to our dictionary via table['key'] = 'value'\n",
    "\n",
    "def standardizeObjects(s):\n",
    "    \"\"\"Takes a string and standardizes it.\"\"\"\n",
    "    \n",
    "    a = []\n",
    "    \n",
    "    for index, value in enumerate(s.split()):\n",
    "        if value.lower() in table:\n",
    "            s = s.replace(value, table[value])\n",
    "        \n",
    "    return s\n",
    "            \n",
    "\n",
    "\n",
    "standardizeObjects('wyd Charlie, dm me')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending upon the usage of preprocessed data, text features can be constructed using various techniques:\n",
    "1. Syntactical Parsing\n",
    "2. Entitles or N-grams\n",
    "3. Word-Based Features\n",
    "4. Statistical Features\n",
    "5. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactical Parsing\n",
    "\n",
    "Syntactical parsing involves analyzing the words in a sentence for their grammar and observing how they are arranged in a sentence. This involves:\n",
    "1. Dependency Grammar\n",
    "2. Part of Speech Tags\n",
    "\n",
    "Dependency Trees - Displays the relationship amongs words in a sentence\n",
    "Part of Speech Tagging - Defines the usage and function of a word in a sentence (nouns, verbs, adjectives, adverbs, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Today', 'NN'), ('I', 'PRP'), ('went', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('gym', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "tokens = word_tokenize(\"Today I went to the gym.\")\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words have multiple meanings according to their usage:\n",
    "- I was stuck in a traffic jam\n",
    "- I like jam on my toast\n",
    "\n",
    "Lesk Algorithm can be used for this type of classification (https://en.wikipedia.org/wiki/Lesk_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving Word-Based Features\n",
    "\n",
    "A learning model could learn different contexts of a word when words are used as features.\n",
    "\n",
    "\"Book my flight, I will read this book.\"\n",
    "\n",
    "Tokens: (“book”, 2), (“my”, 1), (“flight”, 1), (“I”, 1), (“will”, 1), (“read”, 1), (“this”, 1) \n",
    "\n",
    "Tokens with POS: (“book_VB”, 1), (“my_PRP$”, 1), (“flight_NN”, 1), (“I_PRP”, 1), (“will_MD”, 1), (“read_VB”, 1), (“this_DT”, 1), (“book_NN”, 1)\n",
    "\n",
    "#### Normalization and Lemmatization\n",
    "Parts of Speach (POS) tags are the basis of lemmatization for converting a word to its base form (lemma).\n",
    "\n",
    "#### Efficient Stopword Removal\n",
    "POS tags are also useful in the removal of stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction (Entities as Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entities are defined as the most important chunks of a sentence - noun phrases, verb phrases, or both. Entity Detection algorithms are generally ensemble models of rule based parsing, dictionary lookups, pos tagging, and dependency parsing.\n",
    "\n",
    "The appliaction of entity detection can be seen in the automated chat bots, content analyzers, and consumer insights.\n",
    "\n",
    "\"At the W party Thursday night at Chateau Marmont, Cate Blanchett barely made it up the elevator.\"\n",
    "\n",
    "From here we can see that:\n",
    "- Date: Thursday\n",
    "- Time: night\n",
    "- Location: Chateau Marmont\n",
    "- Person: Cate Blanchett"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition (NER)\n",
    "\n",
    "The process of detecting the named entities such as person names, location names, company names, etc. from the text is called as NER. \n",
    "\n",
    "\"Sergey Brin, the manager of Google Inc. is walking in the streets of New York.\"\n",
    "\n",
    "- Named Entities:  \n",
    "    - (“person” : “Sergey Brin” ), (“org” : “Google Inc.”), (“location” : “New York”)\n",
    "    \n",
    "\n",
    "#### Noun Phrase Identification\n",
    "These steps deal with extracting all the noun phrases from a text using dependency parsing and part of speech tagging.\n",
    "\n",
    "#### Phrase Classification\n",
    "This is the classification step in which all the extracted noun phrases are classified into their respective categores (locations, names, etc.).\n",
    "\n",
    "Google Maps API provides a good path to disambiguate locations, dbpedia can be used to extract data from wikipedia (persons/orgs). We can also just create lookup tables and dictionaries by combining information from all different sources.\n",
    "\n",
    "#### Entity Disambiguation\n",
    "Sometimes entities will be misclassified, hence creating a validation layer on top of the results is very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a process of automatically identifying the topics present in a text corpus. It derives the hidden patterns among the words in the corpus in an unsuperivsed manner. Tops are defined as 'a repeating pattern of co-occurring terms in a corpus'.\n",
    "\n",
    "Given a topic of 'Healthcare', a good topic model result set could be:\n",
    "    - 'health', 'doctor', 'patient', 'hospital'\n",
    "\n",
    "The topic of 'Farming' could yield:\n",
    "    - 'farm', 'crops', 'wheat'\n",
    "    \n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is the most popular topic modeling technique. (https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.029*\"driving\" + 0.029*\"My\" + 0.029*\"my\" + 0.029*\"sister\" + 0.029*\"to\" + 0.029*\"and\" + 0.029*\"stress\" + 0.029*\"Doctors\" + 0.029*\"suggest\" + 0.029*\"may\"'), (1, '0.053*\"driving\" + 0.053*\"sister\" + 0.053*\"My\" + 0.053*\"my\" + 0.053*\"to\" + 0.053*\"time\" + 0.053*\"spends\" + 0.053*\"practice.\" + 0.053*\"a\" + 0.053*\"father\"'), (2, '0.063*\"to\" + 0.036*\"but\" + 0.036*\"father.\" + 0.036*\"have\" + 0.036*\"not\" + 0.036*\"likes\" + 0.036*\"sugar,\" + 0.036*\"Sugar\" + 0.036*\"consume.\" + 0.036*\"bad\"')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams as Features\n",
    "\n",
    "A combination of N words together are called N-Grams. N-Grams (N > 1) are generally more informative as compared to words (Unigrams) as features. Bi-Grams (N = 2) are considered the most important features of all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "generate_ngrams('this is a sample text', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Features\n",
    "\n",
    "Text data can also be quantified directly into numbers.\n",
    "\n",
    "\n",
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF-IDF converts the text into vector models based on the occurrence of words in the document without taking into consideration the exact ordering..\n",
    "\n",
    "- Term Frequency (TF): Defined as the count of a term 'T' in a document 'D'.\n",
    "- Inverse Document Frequency (IDF): Logarithmic ratio of total documents available in the corpus and number of documents containing term 'T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.5844829010200651\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 1)\t0.34520501686496574\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (2, 4)\t0.444514311537431\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 5)\t0.5844829010200651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "obj = TfidfVectorizer()\n",
    "\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count/Density/Readability Features\n",
    "\n",
    "Count or Density based features can also be used in models and analysis. \n",
    "1. Word Count\n",
    "2. Sentence Count\n",
    "3. Punctuate Count\n",
    "4. Industry Specific Words\n",
    "\n",
    "Textstat can be used to create such features (https://github.com/shivam5992/textstat)\n",
    "\n",
    "\n",
    "#### Word Embedding (Text Vectors)\n",
    "\n",
    "Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are widely used in deep learning models such as Convolution Neural Networks and Recurrent Neural Networks.\n",
    "\n",
    "Word2Vec and GloVe are two popular models to create word embedding of a text. These models take a text corpus as input and produce the word vectors as output.\n",
    "\n",
    "Word2Vec (https://code.google.com/archive/p/word2vec/)\n",
    "GloVe (https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "Word2Vec model is composed of a preprocessing module, a shallow neural network model called Continuous Bag of Words, and another shallow neural network model called skip-gram. These models are widely used for all other NLP problems. It first constructs a vocabulary from the training corpus and then learns word embedding representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.010777194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninom\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "sentences = [['data', 'science'], ['google', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print(model.wv.similarity('data', 'science'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Tasks of NLP\n",
    "\n",
    "#### Text Classification\n",
    "\n",
    "Text classification is one of the classical problems of NLP.\n",
    "- Identifying email spam\n",
    "- News article classifications\n",
    "\n",
    "Text classification is defined as a technique to systematically classify a text object (document or sentence) in one of the fixed categories. It's especially helpful if the data is large.\n",
    "\n",
    "A typical natural language classifier consists of two parts: \n",
    "1. Training\n",
    "2. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
    "\n",
    "model = NBC(training_corpus) \n",
    "print(model.classify(\"Their codes are amazing.\"))\n",
    "print(model.accuracy(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Class_A' 'Class_A' 'Class_B' 'Class_B' 'Class_A' 'Class_A']\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class_A       0.50      0.67      0.57         3\n",
      "     Class_B       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.50         6\n",
      "   macro avg       0.50      0.50      0.49         6\n",
      "weighted avg       0.50      0.50      0.49         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm \n",
    "\n",
    "# preparing data for SVM model (using the same training_corpus, test_corpus from naive bayes example)\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for row in training_corpus:\n",
    "    train_data.append(row[0])\n",
    "    train_labels.append(row[1])\n",
    "\n",
    "test_data = [] \n",
    "test_labels = [] \n",
    "for row in test_corpus:\n",
    "    test_data.append(row[0]) \n",
    "    test_labels.append(row[1])\n",
    "\n",
    "# Create feature vectors \n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "# Train the feature vectors\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "# Apply model on test data \n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# Perform classification with SVM, kernel=linear \n",
    "model = svm.SVC(kernel='linear') \n",
    "model.fit(train_vectors, train_labels) \n",
    "prediction = model.predict(test_vectors)\n",
    "\n",
    "print(prediction)\n",
    "print('\\n')\n",
    "print (classification_report(test_labels, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Matching / Similarity\n",
    "\n",
    "One of the more important areas of NLP is the matching of text objects to find similarities. Important applications of text matching includes automatic spelling correction, data de-duplication, genome analysis, etc.\n",
    "\n",
    "#### Levenshtein Distance\n",
    "Levenshtein distance between two strings is defined as the minimum number of edits needed to transform one string into the other, with allowable edit operations being insertion, deletion, or substitution of a single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def levenshtein(s1,s2): \n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1 \n",
    "    distances = range(len(s1) + 1) \n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistances = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1]) \n",
    "            else:\n",
    "                 newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) \n",
    "        distances = newDistances \n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshtein(\"analyze\", \"analyse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phonetic Matching\n",
    "A Phonetic matching algorithm takes a keyword as input (person's name, location, etc.) and produces a character string that identifies a set of words that are (roughly) phonetically similar. It is very useful for searching large text corpuses, correcting spelling errors, and matching relevant names. \n",
    "\n",
    "Soundex and Metaphone are two main phonetic algorithms used for this purpose. Python's module Fuzzy is used to compute soundex strings for different words.\n",
    "\n",
    "#### Flexible String Matching\n",
    "A complete text matching system includes different algorithms piplined together to compute a variety of text variations. Regular expressions are really helpful for this purpose as well. Other common techniques include exact string matching, lemmatized matching, and compact matching (which takes care of spaces, punctuation, slang words, etc.).\n",
    "\n",
    "#### Cosine Similarity\n",
    "When the text is representated as vector notation, a general cosine similarity can also be applied in order to measure vectorized similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.630\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "def get_cosine(vec1, vec2):\n",
    "    common = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "   \n",
    "    if not denominator:\n",
    "        return 0.0 \n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text): \n",
    "    words = text.split() \n",
    "    return Counter(words)\n",
    "\n",
    "text1 = 'This is an article on analytics vidhya' \n",
    "text2 = 'article on analytics vidhya is about natural language processing'\n",
    "\n",
    "vector1 = text_to_vector(text1) \n",
    "vector2 = text_to_vector(text2) \n",
    "cosine = get_cosine(vector1, vector2)\n",
    "print('{:.3f}'.format(cosine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Libraries for NLP\n",
    "\n",
    "\n",
    "1. Scikit-learn: Machine learning in Python\n",
    "2. Natural Language Toolkit (NLTK): The complete toolkit for all NLP techniques.\n",
    "3. Pattern – A web mining module for the with tools for NLP and machine learning.\n",
    "4. TextBlob – Easy to use nl p tools API, built on top of NLTK and Pattern.\n",
    "5. spaCy – Industrial strength N LP with Python and Cython.\n",
    "6. Gensim – Topic Modelling for Humans\n",
    "7. Stanford Core NLP – NLP services and packages by Stanford NLP Group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
