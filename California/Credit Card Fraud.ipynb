{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are told that 'V columns may be result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28)'.\n",
    "\n",
    "Time: Number of seconds elapsed between this transaction and the first transaction in the dataset\n",
    "\n",
    "Amount: Transaction amount\n",
    "\n",
    "Class: 1 for fraudulent transactions, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fraud Count: {}\\nNon-Fraud Count: {}'.format(df['Class'].value_counts()[1], df['Class'].value_counts()[0]))\n",
    "print('Fraud: {:.3%}\\nNon-Fraud: {:.3%}'.format(df['Class'].value_counts()[1] / len(df), \n",
    "                                                df['Class'].value_counts()[0] / len(df)))\n",
    "\n",
    "df['Class'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of Amount and Time to determine how skewed these features are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "\n",
    "amount_val = df['Amount'].values\n",
    "time_val = df['Time'].values\n",
    "\n",
    "sns.distplot(amount_val, ax=ax[0], color='r')\n",
    "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "ax[0].set_xlabel('Time (s)')\n",
    "\n",
    "sns.distplot(time_val, ax=ax[1], color='b')\n",
    "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "ax[1].set_xlabel('Amount ($)')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling & Distribution\n",
    "Note how each feature of our dataset is spread differently (ie not every feature is between [0, 10]).\n",
    "\n",
    "In order to not have any bias, we should scale Time and Amount.\n",
    "\n",
    "- StandardScaler will transform the data such that its distribution will have a mean value of 0 and standard deviation of 1. (ie the mean is subtracted and then divided by the standard deviation)\n",
    "- RobustScaler removes the median and scales based on IQR (which makes it less prone to outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# RobustScaler is less prone to outliers.\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "\n",
    "df.drop(['Time', 'Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Sample of Data\n",
    "\n",
    "Only 0.18% of our data contains fraudulent transactions. In order to have our model properly understand fraudulent and non-fraudulent transactions, we will evenly split our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print('No Frauds {:.3%}'.format(df['Class'].value_counts()[0] / len(df)))\n",
    "print('Frauds {:.3%}'.format(df['Class'].value_counts()[1] / len(df)))\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "sss = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print('Train: {}     Test: {}'.format(train_index, test_index))\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "\n",
    "# Turn into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "print('-' * 100)\n",
    "\n",
    "print('\\nLabel Distributions:')\n",
    "print(train_counts_label / len(original_ytrain))\n",
    "print(test_counts_label / len(original_ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Undersampling\n",
    "\n",
    "Random Undersampling removes samples from the majority class, with or without replacement. This is a good technique to alleviate imbalanced datasets, however it may increase the variance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make our classes equivalent in order to have a normal distribution of classes.\n",
    "\n",
    "# First we'll shuffle the data (where frac=1 means 100% of the dataframe will be sampled)\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Number of fraudulent classes (492 rows)\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle df again\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Class'].value_counts().plot(kind='barh', title='Number of Classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have evenly distributed the subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix (Heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pearson Correlation of Features\n",
    "dfCorr = df.corr()\n",
    "new_dfCorr = new_df.corr()\n",
    "\n",
    "\n",
    "# Original df\n",
    "fig, ax = plt.subplots(2, 1, figsize=(24, 24))\n",
    "a = sns.heatmap(dfCorr, cmap='coolwarm',         # Hotter = Positive, Colder = Negative\n",
    "            annot=False,\n",
    "            vmin=-1,                             # set the minimum value of the color scale to -1\n",
    "            linewidths=2,\n",
    "            linecolor='white',\n",
    "           ax=ax[0])\n",
    "bottom, top = a.get_ylim()\n",
    "a.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Pearson Correlation - Imbalanced Dataset\", size=16)\n",
    "a.tick_params(axis='both', which='major', rotation=45, labelsize=10)\n",
    "\n",
    "\n",
    "# New df\n",
    "b = sns.heatmap(new_dfCorr, cmap='coolwarm',     # Hotter = Positive, Colder = Negative\n",
    "            annot=False,\n",
    "            vmin=-1,                             # set the minimum value of the color scale to -1\n",
    "            linewidths=2,\n",
    "            linecolor='white',\n",
    "           ax=ax[1])\n",
    "bottom, top = b.get_ylim()\n",
    "b.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Pearson Correlation - Balanced Dataset\", size=16)\n",
    "b.tick_params(axis='both', which='major', rotation=45, labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the 'Class' row has a few strong correlations. This may indicate fraudulent transactions.\n",
    "\n",
    "- V4\n",
    "- V3\n",
    "- V7\n",
    "- V9\n",
    "- V10\n",
    "- V11\n",
    "- V12\n",
    "- V14\n",
    "- V16\n",
    "- V17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of negative correlation classes\n",
    "fig, ax = plt.subplots(nrows=4, ncols=3, figsize=(24,24))\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V3\", data=new_df, ax=ax[0, 0]).set_title('V3')\n",
    "sns.boxplot(x=\"Class\", y=\"V4\", data=new_df, ax=ax[0, 1]).set_title('V4')\n",
    "sns.boxplot(x=\"Class\", y=\"V7\", data=new_df, ax=ax[0, 2]).set_title('V7')\n",
    "sns.boxplot(x=\"Class\", y=\"V9\", data=new_df, ax=ax[1, 0]).set_title('V9')\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax[1, 1]).set_title('V10')\n",
    "sns.boxplot(x=\"Class\", y=\"V11\", data=new_df, ax=ax[1, 2]).set_title('V11')\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax[2, 0]).set_title('V12')\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, ax=ax[2, 1]).set_title('V14')\n",
    "sns.boxplot(x=\"Class\", y=\"V16\", data=new_df, ax=ax[2, 2]).set_title('V16')\n",
    "sns.boxplot(x=\"Class\", y=\"V17\", data=new_df, ax=ax[3, 0]).set_title('V17')\n",
    "fig.delaxes(ax[3, 1])\n",
    "fig.delaxes(ax[3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal now is to remove outliers from these boxplots. Maybe not all outliers, but at the very least the extreme outliers. This will allow our model to be more accurate.\n",
    "\n",
    "'V14' looks nicely centered and will have a normal distribution. 'V12' and 'V10' seem to look like they will have some imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
    "\n",
    "v14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v14_fraud_dist, ax=ax1, fit=norm, color='#FB8861')\n",
    "ax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "v12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v12_fraud_dist, ax=ax2, fit=norm, color='#56F9BB')\n",
    "ax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "\n",
    "v10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v10_fraud_dist, ax=ax3, fit=norm, color='#C5B3F9')\n",
    "ax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V14\n",
    "# Apply standard IQR of 1.5\n",
    "\n",
    "v14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\n",
    "print('Quartile 25: {:.3f}\\nQuartile 75: {:.3f}'.format(q25, q75))\n",
    "v14_iqr = q75 - q25\n",
    "print('IQR: {:.3f}'.format(v14_iqr))\n",
    "\n",
    "v14_cut_off = v14_iqr * 1.5\n",
    "v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\n",
    "print('\\nCut off: {:.3f}\\nLower Bound: {:.3f}\\nUpper Bound:{:.3f}'.format(v14_cut_off, v14_lower, v14_upper))\n",
    "\n",
    "# new_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\n",
    "print('----' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is great for highly clustered data.\n",
    "\n",
    "PCA and SVD are both very similar. They both revolve around computing the orthogonal transformation that decorrelates the variables and keeps the ones with the largest variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']\n",
    "\n",
    "# t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n",
    "\n",
    "# Truncated SVD \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_tsne[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_pca[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "blue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\n",
    "red_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 4))\n",
    "\n",
    "# t-SNE\n",
    "ax[0].scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=(y==0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax[0].set_title('t-SNE', fontsize=14)\n",
    "ax[0].grid(True)\n",
    "ax[0].legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# PCA\n",
    "ax[1].scatter(X_reduced_pca[:, 0], X_reduced_pca[:, 1], c=(y==0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax[1].set_title('PCA', fontsize=14)\n",
    "ax[1].grid(True)\n",
    "ax[1].legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# SVD\n",
    "ax[2].scatter(X_reduced_svd[:, 0], X_reduced_svd[:, 1], c=(y==0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax[2].set_title('SVD', fontsize=14)\n",
    "ax[2].grid(True)\n",
    "ax[2].legend(handles=[blue_patch, red_patch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how PCA and SVD yield very similar results. This is due to the fact that PCA and SVD both revolve around orthogonal transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Turn the values into arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifiers = {\n",
    "    'LogisticRegression' : LogisticRegression(),\n",
    "    'KNN' : KNeighborsClassifier(),\n",
    "    'SVC' : SVC(),\n",
    "    'DecisionTree' : DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    trainingScore = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    print('{}\\nTraining Score: {:.3%}\\n'.format(key, trainingScore.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV - Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# [LogisticRegression Parameters]\n",
    "log_reg_params = {'penalty' : ['l1', 'l2'],\n",
    "                  'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(X_train, y_train)\n",
    "# LR best parameters\n",
    "log_reg = grid_log_reg.best_estimator_ \n",
    "\n",
    "\n",
    "# [KNN Parameters]\n",
    "knears_params = {'n_neighbors': list(range(2,5,1)), \n",
    "                 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\n",
    "grid_knears.fit(X_train, y_train)\n",
    "# KNN best parameters\n",
    "knears_neighbors = grid_knears.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "# [SVC Parameters]\n",
    "svc_params = {'C': [0.5, 0.7, 0.9, 1], \n",
    "              'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "grid_svc = GridSearchCV(SVC(), svc_params)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "\n",
    "# SVC best parameters\n",
    "svc = grid_svc.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "# [DecisionTree Parameters]\n",
    "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n",
    "              \"min_samples_leaf\": list(range(5,7,1))}\n",
    "grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "\n",
    "# DT best parameters\n",
    "tree_clf = grid_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting Case\n",
    "\n",
    "log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
    "print('Logistic Regression Cross Validation Score: {:.3%}'.format(log_reg_score.mean()))\n",
    "\n",
    "\n",
    "knears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\n",
    "print('Knears Neighbors Cross Validation Score: {:.3%}'.format(knears_score.mean()))\n",
    "\n",
    "svc_score = cross_val_score(svc, X_train, y_train, cv=5)\n",
    "print('Support Vector Classifier Cross Validation Score: {:.3%}'.format(svc_score.mean()))\n",
    "\n",
    "tree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\n",
    "print('DecisionTree Classifier Cross Validation Score: {:.3%}'.format(tree_score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves:\n",
    "\n",
    "- The wider the gap between the training score and the cross validation score, the more likely your model is overfitting (high variance).\n",
    "- If the score is low in both training and cross-validation sets this is an indication that our model is underfitting (high bias)\n",
    "- Logistic Regression Classifier shows the best score in both training and cross-validating sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Undersampling\n",
    "\n",
    "- Randomly remove samples from the majority class, with or without replacement. This is one of the earliest techniques used to alleviate imbalance in the dataset, however, it may increase the variance of the classifier and may potentially discard useful or important samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "# We will undersample during cross validating\n",
    "undersample_X = df.drop('Class', axis=1)\n",
    "undersample_y = df['Class']\n",
    "\n",
    "# Gather data from our KFold cross-validator\n",
    "for train_index, test_index in sss.split(undersample_X, undersample_y):\n",
    "#     print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n",
    "    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n",
    "    \n",
    "    \n",
    "# Set undersampled data to arrays\n",
    "undersample_Xtrain = undersample_Xtrain.values\n",
    "undersample_Xtest = undersample_Xtest.values\n",
    "undersample_ytrain = undersample_ytrain.values\n",
    "undersample_ytest = undersample_ytest.values \n",
    "\n",
    "# Define metrics arrays\n",
    "undersample_accuracy = []\n",
    "undersample_precision = []\n",
    "undersample_recall = []\n",
    "undersample_f1 = []\n",
    "undersample_auc = []\n",
    "\n",
    "# Implementing NearMiss Technique \n",
    "# Distribution of NearMiss (just to see how it distributes the labels we won't use these variables)\n",
    "X_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\n",
    "print('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n",
    "\n",
    "# Cross Validating the right way\n",
    "for train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n",
    "    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n",
    "    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n",
    "    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n",
    "    \n",
    "    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
    "    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Learning Curves\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None, n_jobs=1, \n",
    "                        train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    \n",
    "    # Plot all 4 estimators\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(20, 14), sharey=True)\n",
    "    \n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim) \n",
    "        \n",
    "    \n",
    "    # Estimator 1\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    ax[0, 0].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean,\n",
    "                   train_scores_mean + train_scores_std, alpha=0.1, color=\"#FF9124\")\n",
    "    \n",
    "    ax[0, 0].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean,\n",
    "                   test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492FF\")\n",
    "    \n",
    "    ax[0, 0].plot(train_sizes, train_scores_mean, 'o-', color='#FF9124')\n",
    "    ax[0, 0].plot(train_sizes, test_scores_mean, 'o-', color='#24292FF')\n",
    "    ax[0, 0].set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n",
    "    ax[0, 0].set_xlabel('Training size (m)')\n",
    "    ax[0, 0].set_ylabel('Score')\n",
    "    ax[0, 0].grid(True)\n",
    "    ax[0, 0].legend(loc=\"best\")\n",
    "    \n",
    "    \n",
    "    # Estimator 2\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    ax[0, 1].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean,\n",
    "                   train_scores_mean + train_scores_std, alpha=0.1, color=\"#FF9124\")\n",
    "    \n",
    "    ax[0, 1].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean,\n",
    "                   test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492FF\")\n",
    "    \n",
    "    ax[0, 1].plot(train_sizes, train_scores_mean, 'o-', color='#FF9124')\n",
    "    ax[0, 1].plot(train_sizes, test_scores_mean, 'o-', color='#24292FF')\n",
    "    ax[0, 1].set_title(\"KNN Learning Curve\", fontsize=14)\n",
    "    ax[0, 1].set_xlabel('Training size (m)')\n",
    "    ax[0, 1].set_ylabel('Score')\n",
    "    ax[0, 1].grid(True)\n",
    "    ax[0, 1].legend(loc=\"best\")\n",
    "    \n",
    "    \n",
    "    # Estimator 3\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    ax[1, 0].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean,\n",
    "                   train_scores_mean + train_scores_std, alpha=0.1, color=\"#FF9124\")\n",
    "    \n",
    "    ax[1, 0].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean,\n",
    "                   test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492FF\")\n",
    "    \n",
    "    ax[1, 0].plot(train_sizes, train_scores_mean, 'o-', color='#FF9124')\n",
    "    ax[1, 0].plot(train_sizes, test_scores_mean, 'o-', color='#24292FF')\n",
    "    ax[1, 0].set_title(\"SVM Curve\", fontsize=14)\n",
    "    ax[1, 0].set_xlabel('Training size (m)')\n",
    "    ax[1, 0].set_ylabel('Score')\n",
    "    ax[1, 0].grid(True)\n",
    "    ax[1, 0].legend(loc=\"best\")\n",
    "    \n",
    "    \n",
    "    # Estimator 4\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    ax[1, 1].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean,\n",
    "                   train_scores_mean + train_scores_std, alpha=0.1, color=\"#FF9124\")\n",
    "    \n",
    "    ax[1, 1].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean,\n",
    "                   test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492FF\")\n",
    "    \n",
    "    ax[1, 1].plot(train_sizes, train_scores_mean, 'o-', color='#FF9124')\n",
    "    ax[1, 1].plot(train_sizes, test_scores_mean, 'o-', color='#24292FF')\n",
    "    ax[1, 1].set_title(\"Decision Tree Learning Curve\", fontsize=14)\n",
    "    ax[1, 1].set_xlabel('Training size (m)')\n",
    "    ax[1, 1].set_ylabel('Score')\n",
    "    ax[1, 1].grid(True)\n",
    "    ax[1, 1].legend(loc=\"best\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
    "plot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general behavior we would expect from a learning curve is this:\n",
    "\n",
    "- A model of a given complexity will __overfit__ a small dataset: this means the training score will be relatively high, while the validation score will be relatively low.\n",
    "\n",
    "- A model of a given complexity will __underfit__ a large dataset: this means that the\n",
    "training score will decrease, but the validation score will increase.\n",
    "\n",
    "- A model will never, except by chance, give a better score to the validation set than\n",
    "the training set: this means the curves should keep getting closer together but\n",
    "never cross."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision function is a function which takes a dataset as input and gives a decision as output. What the decision can be depends on the problem at hand. In our scenario, we will attempt to classify an output.\n",
    "\n",
    "- Estimation problems: the \"decision\" is the estimate.\n",
    "- Hypothesis testing problems: the decision is to reject or not reject the null hypothesis.\n",
    "- Classification problems: the decision is to classify a new observation (or observations) into a category.\n",
    "- Model selection problems: the decision is to chose one of the candidate models.\n",
    "\n",
    "\n",
    "### Our goal now is to predict the output from our cross validation as opposed to just the scores. Hence we will use cross_val_predict and not cross_val_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Create dataframe with all scores and classifier names\n",
    "log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5, method='decision_function')\n",
    "\n",
    "knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n",
    "\n",
    "svc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "tree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)\n",
    "\n",
    "\n",
    "\n",
    "# Take predicted results and find roc_auc_score\n",
    "print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\n",
    "print('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\n",
    "print('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\n",
    "print('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\n",
    "knear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\n",
    "svc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\n",
    "tree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n",
    "\n",
    "\n",
    "def graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n",
    "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n",
    "    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n",
    "    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n",
    "    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.01, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                )\n",
    "    plt.legend()\n",
    "    \n",
    "graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positives**: Correctly Classified Fraud Transactions\n",
    "\n",
    "**False Positives**: Incorrectly Classified Fraud Transactions\n",
    "\n",
    "**True Negative**: Correctly Classified Non-Fraud Transactions\n",
    "\n",
    "**False Negative**: Incorrectly Classified Non-Fraud Transactions\n",
    "\n",
    "**Precision**: True Positives/(True Positives + False Positives)\n",
    "\n",
    "**Recall**: True Positives/(True Positives + False Negatives)\n",
    "\n",
    "**Precision** tells us how precise (or sure) our model is detecting fraud transactions while **recall** is the amount of fraud cases our model is able to detect.\n",
    "\n",
    "**Precision/Recall Tradeoff**: The more precise (selective) our model is, the less cases it will detect. \n",
    "\n",
    "Example: Assuming that our model has a precision of 95%, Let's say there are only 5 fraud cases in which the model is 95% precise or more that these are fraud cases. Then let's say there are 5 more cases that our model considers 90% to be a fraud case, if we lower the precision there are more cases that our model will be able to detect.\n",
    "\n",
    "Precision starts to descend between 0.90 and 0.92 nevertheless, our precision score is still pretty high and still we have a descent recall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "precision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)\n",
    "\n",
    "y_pred = log_reg.predict(X_train)\n",
    "\n",
    "\n",
    "# Overfitting Case\n",
    "print('---' * 10)\n",
    "print('Overfitting: \\n')\n",
    "print('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\n",
    "print('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\n",
    "print('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\n",
    "print('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\n",
    "print('---' * 10)\n",
    "\n",
    "# How it should look like\n",
    "print('---' * 10)\n",
    "print('How it should be:\\n')\n",
    "print(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\n",
    "print(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\n",
    "print(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\n",
    "print(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\n",
    "print('---' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted mean of precisions achieved at each threshold (average precision score)\n",
    "\n",
    "undersample_y_score = log_reg.decision_function(original_Xtest)\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "undersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n",
    "print(\"Average precision-recall score: {:.3f}\".format(undersample_average_precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
